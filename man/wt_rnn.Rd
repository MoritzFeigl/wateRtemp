% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wt_rnn.R
\name{wt_rnn}
\alias{wt_rnn}
\title{wt_rnn}
\usage{
wt_rnn(
  train_data,
  test_data = NULL,
  type = "LSTM",
  catchment = NULL,
  model_name = NULL,
  seed = NULL,
  n_iter = 40,
  n_random_initial_points = 20,
  epochs = 100,
  early_stopping_patience = 5,
  ensemble_runs = 5,
  bounds_layers = c(1, 5),
  bounds_units = c(5, 300),
  bounds_dropout = c(0, 0.4),
  bounds_batch_size = c(5, 150),
  bounds_timesteps = c(5, 200),
  initial_grid_from_model_scores = TRUE
)
}
\arguments{
\item{train_data}{Data frame containing training data created by using wt_preprocessing()}

\item{test_data}{Data frame containing test data created by using wt_preprocessing()}

\item{type}{RNN cell type to use. Can be either "LSTM" for the Long short-term model, or "GRU" for the Gated recurrent unit.}

\item{catchment}{Catchment name as string, used for storing results in current working directory.}

\item{model_name}{Name of this particular model run as string, used for storing results in the catchment folder.}

\item{seed}{Random seed.}

\item{n_iter}{Number of iteration steps for bayesian hyperparameter optimization.}

\item{n_random_initial_points}{Number of sampled initial random points for bayesian hyperparameter optimization}

\item{epochs}{integer. Number of training epochs}

\item{early_stopping_patience}{Integer. Early stopping patience, i.e. the number of epochs with no improvement to waite before stopping the training}

\item{ensemble_runs}{Number of ensembles used for making the finel model.}

\item{bounds_layers}{Vector containing the lower and upper bound of the numbers of layers used in the bayesian hyperparameter optimization.}

\item{bounds_units}{Vector containing the lower and upper bound of the numbers of units used in the bayesian hyperparameter optimization.}

\item{bounds_dropout}{Vector containing the lower and upper bound of the numbers of dropout used in the bayesian hyperparameter optimization.}

\item{bounds_batch_size}{Vector containing the lower and upper bound of the numbers of batch size used in the bayesian hyperparameter optimization.}

\item{bounds_timesteps}{Vector containing the lower and upper bound of the numbers of timesteps used in the bayesian hyperparameter optimization.}

\item{initial_grid_from_model_scores}{logical. Should previous results be used as initial grid for the hyperparameter optimization? These have to be stored in the model_name folder under model_scores.csv}
}
\description{
Recurrent neural network implementation for stream water temperature prediction including Bayesian hyperparameter optimization. All results are stored automatically in the folder catchment/model_name.
}
\examples{
\dontrun{
data(Aschach)
wt_preprocess(Aschach)
train_data <- feather::read_feather("Aschach/train_data.feather")
test_data <- feather::read_feather("Aschach/test_data.feather")

wt_rnn(train_data, test_data, "GRU", "Aschach", "standard_rnn_gru")
}
}
